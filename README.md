# Practical Data Engineering

Building data pipelines without sufficient guidance can be overwhelming. It can be particularly hard if you are the sole data engineer and have no one to provide feedback on your pipelines. It can be worse if you are starting out and were trusted with such responsibilities. Balancing deliverables and maintaining best practices can be challenging.

Also, you are expected to have a good amount of business understanding, the answer to why you are building what you are building. A good understanding of the product from an end-to-end user experience is also very important. You need to understand the impact your mistakes have on the business.

A modern data engineer is not only about reading data and storing it at some destination, maybe owing to the sophistication of the tooling that has now taken care of the undifferentiated heavy lifting that we needed to do before, but they are also expected to:

- Actively participate in the orchestration of the pipeline, the design itself.
- Building for security
- Have a fair amount of DevOps knowledge.
- Know more about the product they are building for.

This repo is for housing practical data engineering explorations while we clean up on industry best practice.

## Project 1: Simple ETL Service

This is a very basic example where we use pyspark running on jupyter notebooks to read data off an API and store that data in a postgres database.

### Skills

- Python
- Docker
- Pyspark
- Postgres

### Difficulty

3/10
