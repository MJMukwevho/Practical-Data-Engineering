{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad70e36-0144-4e0c-bdd7-b776ba8c28eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import col\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a55c6a-9191-465e-a710-29c801b16368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/24 17:54:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# master url\n",
    "master_url = \"spark://spark-master:7077\"  # Or use the actual hostname or IP\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyLocalSparkJob\") \\\n",
    "    .master(master_url) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e753550-54f2-464b-a97d-3a4f6863ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(num_rows, num_cols):\n",
    "\n",
    "    #generate column names\n",
    "    column_names = [f\"col{i}\" for i in range(num_cols)]\n",
    "\n",
    "    #generate data\n",
    "    data = [[random.random() for _ in range(num_cols)] for _ in range(num_rows)]\n",
    "\n",
    "    #define schema\n",
    "    schema = StructType([StructField(name, StringType(),True) for name in column_names])\n",
    "\n",
    "    return spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb536fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe(20,5)\n",
    "df2 = create_dataframe(20,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3a678ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18acb05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical plan after adding 5 to col1:\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 5.0) AS new_col1#20]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical plan after adding new_col2:\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 5.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#38]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n",
      "Physical plan after adding new_col3:\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 5.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#38, (cast(col3#3 as double) + 3.0) AS new_col3#58]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n",
      "Final plan\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 5.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#38, (cast(col3#3 as double) + 3.0) AS new_col3#58]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/24 19:19:54 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 1828636 ms exceeds timeout 120000 ms\n",
      "24/10/24 19:19:54 WARN HeartbeatReceiver: Removing executor 0 with no recent heartbeats: 1832329 ms exceeds timeout 120000 ms\n",
      "24/10/24 19:19:54 WARN HeartbeatReceiver: Removing executor 3 with no recent heartbeats: 1828271 ms exceeds timeout 120000 ms\n",
      "24/10/24 19:19:54 ERROR TaskSchedulerImpl: Lost executor 1 on 172.18.0.7: Executor heartbeat timed out after 1828636 ms\n",
      "24/10/24 19:19:54 ERROR TaskSchedulerImpl: Lost executor 0 on 172.18.0.6: Executor heartbeat timed out after 1832329 ms\n",
      "24/10/24 19:19:54 ERROR TaskSchedulerImpl: Lost executor 3 on 172.18.0.3: Executor heartbeat timed out after 1828271 ms\n",
      "24/10/24 19:42:02 WARN HeartbeatReceiver: Removing executor 5 with no recent heartbeats: 316774 ms exceeds timeout 120000 ms\n",
      "24/10/24 19:42:02 WARN HeartbeatReceiver: Removing executor 4 with no recent heartbeats: 316430 ms exceeds timeout 120000 ms\n",
      "24/10/24 19:42:02 ERROR TaskSchedulerImpl: Lost executor 5 on 172.18.0.6: Executor heartbeat timed out after 316774 ms\n",
      "24/10/24 19:42:02 ERROR TaskSchedulerImpl: Lost executor 4 on 172.18.0.7: Executor heartbeat timed out after 316430 ms\n",
      "24/10/25 12:00:07 WARN HeartbeatReceiver: Removing executor 8 with no recent heartbeats: 258245 ms exceeds timeout 120000 ms\n",
      "24/10/25 12:00:07 WARN HeartbeatReceiver: Removing executor 7 with no recent heartbeats: 259505 ms exceeds timeout 120000 ms\n",
      "24/10/25 12:00:07 WARN HeartbeatReceiver: Removing executor 6 with no recent heartbeats: 257551 ms exceeds timeout 120000 ms\n",
      "24/10/25 12:00:07 ERROR TaskSchedulerImpl: Lost executor 8 on 172.18.0.7: Executor heartbeat timed out after 258245 ms\n",
      "24/10/25 12:00:07 ERROR TaskSchedulerImpl: Lost executor 7 on 172.18.0.6: Executor heartbeat timed out after 259505 ms\n",
      "24/10/25 12:00:07 ERROR TaskSchedulerImpl: Lost executor 6 on 172.18.0.3: Executor heartbeat timed out after 257551 ms\n",
      "24/10/26 01:21:03 WARN HeartbeatReceiver: Removing executor 9 with no recent heartbeats: 1232215 ms exceeds timeout 120000 ms\n",
      "24/10/26 01:21:03 WARN HeartbeatReceiver: Removing executor 10 with no recent heartbeats: 1233377 ms exceeds timeout 120000 ms\n",
      "24/10/26 01:21:03 ERROR TaskSchedulerImpl: Lost executor 9 on 172.18.0.7: Executor heartbeat timed out after 1232215 ms\n",
      "24/10/26 01:21:03 ERROR TaskSchedulerImpl: Lost executor 10 on 172.18.0.6: Executor heartbeat timed out after 1233377 ms\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of columns to be transformed\n",
    "columns = ['col1', 'col2', 'col3']\n",
    "\n",
    "# Loop to add 1 to each column and create new columns\n",
    "for i, column in enumerate(columns, start=1):\n",
    "    if column == 'col1':\n",
    "        # Perform the special action: add 5 to col1\n",
    "        df = df.withColumn(f\"new_col{i}\", col(column) + 5)\n",
    "        print(f\"Physical plan after adding 5 to {column}:\")\n",
    "    else:\n",
    "        # For other columns, just add i\n",
    "        df = df.withColumn(f\"new_col{i}\", col(column) + i)\n",
    "        print(f\"Physical plan after adding new_col{i}:\")\n",
    "    \n",
    "    # Force execution by calling an action, e.g., `count()`\n",
    "    df.explain()\n",
    "    df.count()  # Forces execution of the physical plan\n",
    "\n",
    "print(\"Final plan\")\n",
    "df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c50afb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 1.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#27, (cast(col3#3 as double) + 3.0) AS new_col3#35]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of columns to be transformed\n",
    "columns = ['col1', 'col2', 'col3']\n",
    "\n",
    "# Loop to add 1 to each column and create new columns\n",
    "for i, column in enumerate(columns, start=1):\n",
    "    df = df.withColumn(f\"new_col{i}\", col(column) + i)\n",
    "\n",
    "# Show the physical plan\n",
    "df.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c55681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical plan after adding new_col1:\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#209, col1#210, col2#211, col3#212, col4#213, (cast(col1#210 as double) + 1.0) AS new_col1#313, (cast(col2#211 as double) + 2.0) AS new_col2#295, (cast(col3#212 as double) + 3.0) AS new_col3#304]\n",
      "+- *(1) Scan ExistingRDD[col0#209,col1#210,col2#211,col3#212,col4#213]\n",
      "\n",
      "\n",
      "Physical plan after adding new_col2:\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#209, col1#210, col2#211, col3#212, col4#213, (cast(col1#210 as double) + 1.0) AS new_col1#313, (cast(col2#211 as double) + 2.0) AS new_col2#322, (cast(col3#212 as double) + 3.0) AS new_col3#304]\n",
      "+- *(1) Scan ExistingRDD[col0#209,col1#210,col2#211,col3#212,col4#213]\n",
      "\n",
      "\n",
      "Physical plan after adding new_col3:\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#209, col1#210, col2#211, col3#212, col4#213, (cast(col1#210 as double) + 1.0) AS new_col1#313, (cast(col2#211 as double) + 2.0) AS new_col2#322, (cast(col3#212 as double) + 3.0) AS new_col3#331]\n",
      "+- *(1) Scan ExistingRDD[col0#209,col1#210,col2#211,col3#212,col4#213]\n",
      "\n",
      "\n",
      "Final plan\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#209, col1#210, col2#211, col3#212, col4#213, (cast(col1#210 as double) + 1.0) AS new_col1#313, (cast(col2#211 as double) + 2.0) AS new_col2#322, (cast(col3#212 as double) + 3.0) AS new_col3#331]\n",
      "+- *(1) Scan ExistingRDD[col0#209,col1#210,col2#211,col3#212,col4#213]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of columns to be transformed\n",
    "columns = ['col1', 'col2', 'col3']\n",
    "\n",
    "# Loop to add 1 to each column and create new columns\n",
    "for i, column in enumerate(columns, start=1):\n",
    "    df = df.withColumn(f\"new_col{i}\", col(column) + i)\n",
    "    print(f\"Physical plan after adding new_col{i}:\")\n",
    "    df.explain()  \n",
    "\n",
    "print(\"Final plan\")\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bce40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [col0#209, col1#210, col2#211, col3#212, col4#213, (cast(col1#210 as double) + 1.0) AS new_col1#239, (cast(col2#211 as double) + 2.0) AS new_col2#257, (cast(col3#212 as double) + 3.0) AS new_col3#277]\n",
      "+- *(1) Scan ExistingRDD[col0#209,col1#210,col2#211,col3#212,col4#213]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d843d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(raw_text):\n",
    "    # Decode the byte string to a normal string\n",
    "    decoded_text = raw_text.decode('utf-8')\n",
    "\n",
    "    # Print each line in a more readable format\n",
    "    formatted_text = decoded_text.replace(' | ', '\\n | ')\n",
    "\n",
    "    # Print the result\n",
    "    print(formatted_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9607f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) MapPartitionsRDD[13] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[12] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  SQLExecutionRDD[11] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[10] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n",
      "\n",
      " |  MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n",
      "\n",
      " |  PythonRDD[1] at RDD at PythonRDD.scala:53 []\n",
      "\n",
      " |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\n"
     ]
    }
   ],
   "source": [
    "format_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef924f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d83a191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [col1#1, col2#2]\n",
      "+- *(1) Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND (cast(col2#2 as double) > 0.2)))\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\n",
    "  \"col1\",\n",
    "   \"col2\") \\\n",
    "   .filter(col(\"col1\")>0.2)\\\n",
    "    .filter(col(\"col2\")>0.2)\n",
    "\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "959e9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_text = df.rdd.toDebugString()\n",
    "# format_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b6c1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8) MapPartitionsRDD[23] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[22] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  SQLExecutionRDD[21] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[20] at javaToPython at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[4] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\n",
      "\n",
      " |  MapPartitionsRDD[3] at map at SerDeUtil.scala:69 []\n",
      "\n",
      " |  MapPartitionsRDD[2] at mapPartitions at SerDeUtil.scala:117 []\n",
      "\n",
      " |  PythonRDD[1] at RDD at PythonRDD.scala:53 []\n",
      "\n",
      " |  ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289 []\n"
     ]
    }
   ],
   "source": [
    "df = df \\\n",
    "       .withColumn(\"col6\", col(\"col1\") + col(\"col2\")) \\\n",
    "       .filter((col(\"col1\") > 0.2) & (col(\"col2\") > 0.3))\n",
    "\n",
    "raw_text = df.rdd.toDebugString()\n",
    "format_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4417f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    df = df.withColumn(col, df[col].cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70cc4507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cast(col1#1 as float) AS col1#85, cast(col2#2 as float) AS col2#89, cast((cast(col1#1 as double) + cast(col2#2 as double)) as float) AS col6#93]\n",
      "+- *(1) Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND ((cast(col2#2 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))))\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a152f45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, new_col3#35, ('col1 + 4) AS new_col4#44]\n",
      "+- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, (cast(col1#1 as double) + cast(3 as double)) AS new_col3#35]\n",
      "   +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, (cast(col2#2 as double) + cast(2 as double)) AS new_col2#27]\n",
      "      +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + cast(1 as double)) AS new_col1#20]\n",
      "         +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col0: string, col1: string, col2: string, col3: string, col4: string, new_col1: double, new_col2: double, new_col3: double, new_col4: double\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, new_col3#35, (cast(col1#1 as double) + cast(4 as double)) AS new_col4#44]\n",
      "+- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, (cast(col1#1 as double) + cast(3 as double)) AS new_col3#35]\n",
      "   +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, (cast(col2#2 as double) + cast(2 as double)) AS new_col2#27]\n",
      "      +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + cast(1 as double)) AS new_col1#20]\n",
      "         +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 1.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#27, (cast(col1#1 as double) + 3.0) AS new_col3#35, (cast(col1#1 as double) + 4.0) AS new_col4#44]\n",
      "+- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 1.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#27, (cast(col1#1 as double) + 3.0) AS new_col3#35, (cast(col1#1 as double) + 4.0) AS new_col4#44]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inefficient method chaining\n",
    "df = df.withColumn(\"new_col1\", col(\"col1\") + 1) \\\n",
    "       .withColumn(\"new_col2\", col(\"col2\") + 2) \\\n",
    "       .withColumn(\"new_col3\", col(\"col1\") + 3) \\\n",
    "       .withColumn(\"new_col4\", col(\"col1\") + 4)\n",
    "\n",
    "# Show the physical plan\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c58ded55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#55, new_col2#66, new_col3#77, add_one_udf(col1#1)#87 AS new_col4#88]\n",
      "+- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#55, new_col2#66, add_one_udf(col1#1)#76 AS new_col3#77, new_col4#44]\n",
      "   +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#55, add_one_udf(col2#2)#65 AS new_col2#66, new_col3#35, new_col4#44]\n",
      "      +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, add_one_udf(col1#1)#54 AS new_col1#55, new_col2#27, new_col3#35, new_col4#44]\n",
      "         +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, new_col3#35, (cast(col1#1 as double) + cast(4 as double)) AS new_col4#44]\n",
      "            +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, (cast(col1#1 as double) + cast(3 as double)) AS new_col3#35]\n",
      "               +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, (cast(col2#2 as double) + cast(2 as double)) AS new_col2#27]\n",
      "                  +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + cast(1 as double)) AS new_col1#20]\n",
      "                     +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col0: string, col1: string, col2: string, col3: string, col4: string, new_col1: double, new_col2: double, new_col3: double, new_col4: double\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#55, new_col2#66, new_col3#77, add_one_udf(col1#1)#87 AS new_col4#88]\n",
      "+- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#55, new_col2#66, add_one_udf(col1#1)#76 AS new_col3#77, new_col4#44]\n",
      "   +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#55, add_one_udf(col2#2)#65 AS new_col2#66, new_col3#35, new_col4#44]\n",
      "      +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, add_one_udf(col1#1)#54 AS new_col1#55, new_col2#27, new_col3#35, new_col4#44]\n",
      "         +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, new_col3#35, (cast(col1#1 as double) + cast(4 as double)) AS new_col4#44]\n",
      "            +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, (cast(col1#1 as double) + cast(3 as double)) AS new_col3#35]\n",
      "               +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, (cast(col2#2 as double) + cast(2 as double)) AS new_col2#27]\n",
      "                  +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + cast(1 as double)) AS new_col1#20]\n",
      "                     +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, pythonUDF0#98 AS new_col1#55, pythonUDF1#99 AS new_col2#66, pythonUDF0#98 AS new_col3#77, pythonUDF0#98 AS new_col4#88]\n",
      "+- BatchEvalPython [add_one_udf(col1#1)#54, add_one_udf(col2#2)#65], [pythonUDF0#98, pythonUDF1#99]\n",
      "   +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [col0#0, col1#1, col2#2, col3#3, col4#4, pythonUDF0#98 AS new_col1#55, pythonUDF1#99 AS new_col2#66, pythonUDF0#98 AS new_col3#77, pythonUDF0#98 AS new_col4#88]\n",
      "+- BatchEvalPython [add_one_udf(col1#1)#54, add_one_udf(col2#2)#65], [pythonUDF0#98, pythonUDF1#99]\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Sample UDF to simulate a complex transformation\n",
    "def add_one_udf(col_val):\n",
    "    return col_val + 1\n",
    "\n",
    "# Register the UDF\n",
    "add_one = udf(add_one_udf, DoubleType())\n",
    "\n",
    "# Inefficient chaining using UDFs\n",
    "df = df.withColumn(\"new_col1\", add_one(df[\"col1\"])) \\\n",
    "       .withColumn(\"new_col2\", add_one(df[\"col2\"])) \\\n",
    "       .withColumn(\"new_col3\", add_one(df[\"col1\"])) \\\n",
    "       .withColumn(\"new_col4\", add_one(df[\"col1\"]))\n",
    "\n",
    "# Show the physical plan\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523b6af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, ('col3 + 3) AS new_col3#35]\n",
      "+- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, (cast(col2#2 as double) + cast(2 as double)) AS new_col2#27]\n",
      "   +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + cast(1 as double)) AS new_col1#20]\n",
      "      +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col0: string, col1: string, col2: string, col3: string, col4: string, new_col1: double, new_col2: double, new_col3: double\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, new_col2#27, (cast(col3#3 as double) + cast(3 as double)) AS new_col3#35]\n",
      "+- Project [col0#0, col1#1, col2#2, col3#3, col4#4, new_col1#20, (cast(col2#2 as double) + cast(2 as double)) AS new_col2#27]\n",
      "   +- Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + cast(1 as double)) AS new_col1#20]\n",
      "      +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 1.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#27, (cast(col3#3 as double) + 3.0) AS new_col3#35]\n",
      "+- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [col0#0, col1#1, col2#2, col3#3, col4#4, (cast(col1#1 as double) + 1.0) AS new_col1#20, (cast(col2#2 as double) + 2.0) AS new_col2#27, (cast(col3#3 as double) + 3.0) AS new_col3#35]\n",
      "+- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of columns to be transformed\n",
    "columns = ['col1', 'col2', 'col3']\n",
    "\n",
    "# Loop to add 1 to each column and create new columns\n",
    "for i, column in enumerate(columns, start=1):\n",
    "    df = df.withColumn(f\"new_col{i}\", col(column) + i)\n",
    "\n",
    "# Show the physical plan\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760c3a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (('col1 > 0.2) AND ('col2 > 0.2))\n",
      "+- Project [col1#1, col2#2]\n",
      "   +- Filter (cast(col2#2 as double) > 0.2)\n",
      "      +- Filter (cast(col1#1 as double) > 0.2)\n",
      "         +- Project [col1#1, col2#2]\n",
      "            +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col1: string, col2: string\n",
      "Filter ((cast(col1#1 as double) > 0.2) AND (cast(col2#2 as double) > 0.2))\n",
      "+- Project [col1#1, col2#2]\n",
      "   +- Filter (cast(col2#2 as double) > 0.2)\n",
      "      +- Filter (cast(col1#1 as double) > 0.2)\n",
      "         +- Project [col1#1, col2#2]\n",
      "            +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col1#1, col2#2]\n",
      "+- Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND (cast(col2#2 as double) > 0.2)))\n",
      "   +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [col1#1, col2#2]\n",
      "+- *(1) Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND (cast(col2#2 as double) > 0.2)))\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df2 = df.select(\"col1\", \"col2\") \\\n",
    "       .filter((col(\"col1\") > 0.2) & (col(\"col2\") > 0.2))\n",
    "\n",
    "df2.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "420e1823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (('col1 > 0.2) AND ('col2 > 0.3))\n",
      "+- Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#43]\n",
      "   +- Project [col1#1, col2#2]\n",
      "      +- Filter (cast(col2#2 as double) > 0.2)\n",
      "         +- Filter (cast(col1#1 as double) > 0.2)\n",
      "            +- Project [col1#1, col2#2]\n",
      "               +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col1: string, col2: string, col6: double\n",
      "Filter ((cast(col1#1 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))\n",
      "+- Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#43]\n",
      "   +- Project [col1#1, col2#2]\n",
      "      +- Filter (cast(col2#2 as double) > 0.2)\n",
      "         +- Filter (cast(col1#1 as double) > 0.2)\n",
      "            +- Project [col1#1, col2#2]\n",
      "               +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#43]\n",
      "+- Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND ((cast(col2#2 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))))\n",
      "   +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#43]\n",
      "+- *(1) Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND ((cast(col2#2 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))))\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df3 = df.select(\"col1\", \"col2\") \\\n",
    "       .withColumn(\"col6\", col(\"col1\") + col(\"col2\")) \\\n",
    "       .filter((col(\"col1\") > 0.2) & (col(\"col2\") > 0.3)) \\\n",
    "\n",
    "\n",
    "df3.explain(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df31ad47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter (('col1 > 0.2) AND ('col2 > 0.3))\n",
      "+- Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#47]\n",
      "   +- Filter (cast(col2#2 as double) > 0.2)\n",
      "      +- Filter (cast(col1#1 as double) > 0.2)\n",
      "         +- Project [col1#1, col2#2]\n",
      "            +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col1: string, col2: string, col6: double\n",
      "Filter ((cast(col1#1 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))\n",
      "+- Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#47]\n",
      "   +- Filter (cast(col2#2 as double) > 0.2)\n",
      "      +- Filter (cast(col1#1 as double) > 0.2)\n",
      "         +- Project [col1#1, col2#2]\n",
      "            +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#47]\n",
      "+- Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND ((cast(col2#2 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))))\n",
      "   +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [col1#1, col2#2, (cast(col1#1 as double) + cast(col2#2 as double)) AS col6#47]\n",
      "+- *(1) Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.2) AND ((cast(col2#2 as double) > 0.2) AND (cast(col2#2 as double) > 0.3))))\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df.select(\"col1\", \"col2\",\n",
    "       (col(\"col1\") + col(\"col2\")).alias(\"col6\") )\\\n",
    "       .filter((col(\"col1\") > 0.2) & (col(\"col2\") > 0.3)) \\\n",
    "       \n",
    "\n",
    "df4.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90b882a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) MapPartitionsRDD[36] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[35] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  SQLExecutionRDD[34] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[33] at javaToPython at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[11] at applySchemaToPythonRDD at NativeMethodAccessorImpl.java:0 []\\n |  MapPartitionsRDD[10] at map at SerDeUtil.scala:69 []\\n |  MapPartitionsRDD[9] at mapPartitions at SerDeUtil.scala:117 []\\n |  PythonRDD[8] at RDD at PythonRDD.scala:53 []\\n |  ParallelCollectionRDD[7] at readRDDFromFile at PythonRDD.scala:289 []'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.toDebugString()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5667797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Filter ('col2 > 0.4)\n",
      "+- Project [col1#1, col2#2]\n",
      "   +- Filter (cast(col1#1 as double) > 0.4)\n",
      "      +- Project [col1#1, col2#2]\n",
      "         +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "col1: string, col2: string\n",
      "Filter (cast(col2#2 as double) > 0.4)\n",
      "+- Project [col1#1, col2#2]\n",
      "   +- Filter (cast(col1#1 as double) > 0.4)\n",
      "      +- Project [col1#1, col2#2]\n",
      "         +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [col1#1, col2#2]\n",
      "+- Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.4) AND (cast(col2#2 as double) > 0.4)))\n",
      "   +- LogicalRDD [col0#0, col1#1, col2#2, col3#3, col4#4], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [col1#1, col2#2]\n",
      "+- *(1) Filter ((isnotnull(col1#1) AND isnotnull(col2#2)) AND ((cast(col1#1 as double) > 0.4) AND (cast(col2#2 as double) > 0.4)))\n",
      "   +- *(1) Scan ExistingRDD[col0#0,col1#1,col2#2,col3#3,col4#4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for column in [\"col1\", \"col2\"]:\n",
    "    df = df.select(\n",
    "    \"col1\",\n",
    "    \"col2\") \\\n",
    "    .filter(col(column)>0.4)\\\n",
    "   \n",
    "\n",
    "df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd982170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 19:15:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+----------------+----------------+--------------------+-------------------+--------------------+-------------------+--------------------+-------------------+\n",
      "|customer_id|transaction_amount|transaction_date|transaction_type|transaction_amount_1|discounted_amount_1|transaction_amount_2|discounted_amount_2|transaction_amount_3|discounted_amount_3|\n",
      "+-----------+------------------+----------------+----------------+--------------------+-------------------+--------------------+-------------------+--------------------+-------------------+\n",
      "|1          |100.0             |2024-01-01      |debit           |101.0               |100.0              |102.0               |100.0              |103.0               |100.0              |\n",
      "|2          |200.0             |2024-01-02      |credit          |201.0               |200.0              |202.0               |200.0              |203.0               |200.0              |\n",
      "|3          |150.0             |2024-01-03      |debit           |151.0               |150.0              |152.0               |150.0              |153.0               |150.0              |\n",
      "|4          |300.0             |2024-01-04      |credit          |301.0               |270.0              |302.0               |270.0              |303.0               |270.0              |\n",
      "+-----------+------------------+----------------+----------------+--------------------+-------------------+--------------------+-------------------+--------------------+-------------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, transaction_amount_2#22, discounted_amount_2#31, transaction_amount_3#40, apply_discount(transaction_amount#1)#50 AS discounted_amount_3#51]\n",
      "+- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, transaction_amount_2#22, discounted_amount_2#31, (transaction_amount#1 + cast(3 as double)) AS transaction_amount_3#40]\n",
      "   +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, transaction_amount_2#22, apply_discount(transaction_amount#1)#30 AS discounted_amount_2#31]\n",
      "      +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, (transaction_amount#1 + cast(2 as double)) AS transaction_amount_2#22]\n",
      "         +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, apply_discount(transaction_amount#1)#14 AS discounted_amount_1#15]\n",
      "            +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, (transaction_amount#1 + cast(1 as double)) AS transaction_amount_1#8]\n",
      "               +- LogicalRDD [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: bigint, transaction_amount: double, transaction_date: string, transaction_type: string, transaction_amount_1: double, discounted_amount_1: double, transaction_amount_2: double, discounted_amount_2: double, transaction_amount_3: double, discounted_amount_3: double\n",
      "Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, transaction_amount_2#22, discounted_amount_2#31, transaction_amount_3#40, apply_discount(transaction_amount#1)#50 AS discounted_amount_3#51]\n",
      "+- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, transaction_amount_2#22, discounted_amount_2#31, (transaction_amount#1 + cast(3 as double)) AS transaction_amount_3#40]\n",
      "   +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, transaction_amount_2#22, apply_discount(transaction_amount#1)#30 AS discounted_amount_2#31]\n",
      "      +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, discounted_amount_1#15, (transaction_amount#1 + cast(2 as double)) AS transaction_amount_2#22]\n",
      "         +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, transaction_amount_1#8, apply_discount(transaction_amount#1)#14 AS discounted_amount_1#15]\n",
      "            +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, (transaction_amount#1 + cast(1 as double)) AS transaction_amount_1#8]\n",
      "               +- LogicalRDD [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, (transaction_amount#1 + 1.0) AS transaction_amount_1#8, pythonUDF0#104 AS discounted_amount_1#15, (transaction_amount#1 + 2.0) AS transaction_amount_2#22, pythonUDF0#104 AS discounted_amount_2#31, (transaction_amount#1 + 3.0) AS transaction_amount_3#40, pythonUDF0#104 AS discounted_amount_3#51]\n",
      "+- BatchEvalPython [apply_discount(transaction_amount#1)#14], [pythonUDF0#104]\n",
      "   +- LogicalRDD [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(2) Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, (transaction_amount#1 + 1.0) AS transaction_amount_1#8, pythonUDF0#104 AS discounted_amount_1#15, (transaction_amount#1 + 2.0) AS transaction_amount_2#22, pythonUDF0#104 AS discounted_amount_2#31, (transaction_amount#1 + 3.0) AS transaction_amount_3#40, pythonUDF0#104 AS discounted_amount_3#51]\n",
      "+- BatchEvalPython [apply_discount(transaction_amount#1)#14], [pythonUDF0#104]\n",
      "   +- *(1) Scan ExistingRDD[customer_id#0L,transaction_amount#1,transaction_date#2,transaction_type#3]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35732)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/local/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "import datetime\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Inefficient Transformations Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [\n",
    "    (1, 100.0, \"2024-01-01\", \"debit\"),\n",
    "    (2, 200.0, \"2024-01-02\", \"credit\"),\n",
    "    (3, 150.0, \"2024-01-03\", \"debit\"),\n",
    "    (4, 300.0, \"2024-01-04\", \"credit\"),\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"transaction_amount\", \"transaction_date\", \"transaction_type\"])\n",
    "\n",
    "# Define a UDF for some complex logic (e.g., applying a discount)\n",
    "def apply_discount(amount):\n",
    "    if amount > 250:\n",
    "        return amount * 0.9  # 10% discount\n",
    "    return amount\n",
    "\n",
    "discount_udf = udf(apply_discount, DoubleType())\n",
    "\n",
    "# Inefficient transformation using for loop\n",
    "for i in range(1, 4):\n",
    "    df = df.withColumn(f\"transaction_amount_{i}\", df[\"transaction_amount\"] + i)  # Adding a constant\n",
    "    df = df.withColumn(f\"discounted_amount_{i}\", discount_udf(df[\"transaction_amount\"]))  # Applying discount\n",
    "\n",
    "# Show the final DataFrame\n",
    "df.show(truncate=False)\n",
    "\n",
    "# Show the query plan\n",
    "df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e01f6fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/14 19:18:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+------------+-----------------+--------+------------+\n",
      "|customer_id|membership_status|total_amount|transaction_count|discount|final_amount|\n",
      "+-----------+-----------------+------------+-----------------+--------+------------+\n",
      "|1          |gold             |300.0       |2                |30.0    |270.0       |\n",
      "|2          |silver           |150.0       |1                |7.5     |142.5       |\n",
      "|3          |gold             |300.0       |1                |30.0    |270.0       |\n",
      "+-----------+-----------------+------------+-----------------+--------+------------+\n",
      "\n",
      "== Parsed Logical Plan ==\n",
      "'Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, discount#34, ('total_amount - 'discount) AS final_amount#40]\n",
      "+- Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, CASE WHEN (membership_status#9 = gold) THEN (total_amount#27 * 0.1) WHEN (membership_status#9 = silver) THEN (total_amount#27 * 0.05) ELSE cast(0 as double) END AS discount#34]\n",
      "   +- Aggregate [customer_id#0L, membership_status#9], [customer_id#0L, membership_status#9, sum(transaction_amount#1) AS total_amount#27, count(transaction_type#3) AS transaction_count#29L]\n",
      "      +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, membership_status#9, registration_date#10]\n",
      "         +- Join Inner, (customer_id#0L = customer_id#8L)\n",
      "            :- LogicalRDD [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3], false\n",
      "            +- LogicalRDD [customer_id#8L, membership_status#9, registration_date#10], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "customer_id: bigint, membership_status: string, total_amount: double, transaction_count: bigint, discount: double, final_amount: double\n",
      "Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, discount#34, (total_amount#27 - discount#34) AS final_amount#40]\n",
      "+- Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, CASE WHEN (membership_status#9 = gold) THEN (total_amount#27 * 0.1) WHEN (membership_status#9 = silver) THEN (total_amount#27 * 0.05) ELSE cast(0 as double) END AS discount#34]\n",
      "   +- Aggregate [customer_id#0L, membership_status#9], [customer_id#0L, membership_status#9, sum(transaction_amount#1) AS total_amount#27, count(transaction_type#3) AS transaction_count#29L]\n",
      "      +- Project [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3, membership_status#9, registration_date#10]\n",
      "         +- Join Inner, (customer_id#0L = customer_id#8L)\n",
      "            :- LogicalRDD [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3], false\n",
      "            +- LogicalRDD [customer_id#8L, membership_status#9, registration_date#10], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, discount#34, (total_amount#27 - discount#34) AS final_amount#40]\n",
      "+- Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, CASE WHEN (membership_status#9 = gold) THEN (total_amount#27 * 0.1) WHEN (membership_status#9 = silver) THEN (total_amount#27 * 0.05) ELSE 0.0 END AS discount#34]\n",
      "   +- Aggregate [customer_id#0L, membership_status#9], [customer_id#0L, membership_status#9, sum(transaction_amount#1) AS total_amount#27, count(transaction_type#3) AS transaction_count#29L]\n",
      "      +- Project [customer_id#0L, transaction_amount#1, transaction_type#3, membership_status#9]\n",
      "         +- Join Inner, (customer_id#0L = customer_id#8L)\n",
      "            :- Project [customer_id#0L, transaction_amount#1, transaction_type#3]\n",
      "            :  +- Filter isnotnull(customer_id#0L)\n",
      "            :     +- LogicalRDD [customer_id#0L, transaction_amount#1, transaction_date#2, transaction_type#3], false\n",
      "            +- Project [customer_id#8L, membership_status#9]\n",
      "               +- Filter isnotnull(customer_id#8L)\n",
      "                  +- LogicalRDD [customer_id#8L, membership_status#9, registration_date#10], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, discount#34, (total_amount#27 - discount#34) AS final_amount#40]\n",
      "   +- Project [customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L, CASE WHEN (membership_status#9 = gold) THEN (total_amount#27 * 0.1) WHEN (membership_status#9 = silver) THEN (total_amount#27 * 0.05) ELSE 0.0 END AS discount#34]\n",
      "      +- HashAggregate(keys=[customer_id#0L, membership_status#9], functions=[sum(transaction_amount#1), count(transaction_type#3)], output=[customer_id#0L, membership_status#9, total_amount#27, transaction_count#29L])\n",
      "         +- HashAggregate(keys=[customer_id#0L, membership_status#9], functions=[partial_sum(transaction_amount#1), partial_count(transaction_type#3)], output=[customer_id#0L, membership_status#9, sum#67, count#68L])\n",
      "            +- Project [customer_id#0L, transaction_amount#1, transaction_type#3, membership_status#9]\n",
      "               +- SortMergeJoin [customer_id#0L], [customer_id#8L], Inner\n",
      "                  :- Sort [customer_id#0L ASC NULLS FIRST], false, 0\n",
      "                  :  +- Exchange hashpartitioning(customer_id#0L, 200), ENSURE_REQUIREMENTS, [plan_id=231]\n",
      "                  :     +- Project [customer_id#0L, transaction_amount#1, transaction_type#3]\n",
      "                  :        +- Filter isnotnull(customer_id#0L)\n",
      "                  :           +- Scan ExistingRDD[customer_id#0L,transaction_amount#1,transaction_date#2,transaction_type#3]\n",
      "                  +- Sort [customer_id#8L ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(customer_id#8L, 200), ENSURE_REQUIREMENTS, [plan_id=232]\n",
      "                        +- Project [customer_id#8L, membership_status#9]\n",
      "                           +- Filter isnotnull(customer_id#8L)\n",
      "                              +- Scan ExistingRDD[customer_id#8L,membership_status#9,registration_date#10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ComplexExample\").getOrCreate()\n",
    "\n",
    "# Sample data for transactions\n",
    "transactions_data = [\n",
    "    (1, 100.0, \"2024-01-01\", \"purchase\"),\n",
    "    (1, 200.0, \"2024-01-02\", \"purchase\"),\n",
    "    (2, 150.0, \"2024-01-01\", \"refund\"),\n",
    "    (3, 300.0, \"2024-01-01\", \"purchase\"),\n",
    "]\n",
    "\n",
    "# Sample data for customers\n",
    "customers_data = [\n",
    "    (1, \"gold\", \"2023-01-01\"),\n",
    "    (2, \"silver\", \"2023-05-01\"),\n",
    "    (3, \"gold\", \"2022-01-01\"),\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "transactions_df = spark.createDataFrame(transactions_data, [\"customer_id\", \"transaction_amount\", \"transaction_date\", \"transaction_type\"])\n",
    "customers_df = spark.createDataFrame(customers_data, [\"customer_id\", \"membership_status\", \"registration_date\"])\n",
    "\n",
    "# Perform the transformations\n",
    "result_df = (transactions_df\n",
    "    .join(customers_df, on=\"customer_id\", how=\"inner\")\n",
    "    .groupBy(\"customer_id\", \"membership_status\")\n",
    "    .agg(\n",
    "        F.sum(\"transaction_amount\").alias(\"total_amount\"),\n",
    "        F.count(\"transaction_type\").alias(\"transaction_count\")\n",
    "    )\n",
    "    .withColumn(\"discount\", \n",
    "        F.when(F.col(\"membership_status\") == \"gold\", F.col(\"total_amount\") * 0.1)  # 10% discount for gold members\n",
    "         .when(F.col(\"membership_status\") == \"silver\", F.col(\"total_amount\") * 0.05)  # 5% discount for silver members\n",
    "         .otherwise(0)\n",
    "    )\n",
    "    .withColumn(\"final_amount\", F.col(\"total_amount\") - F.col(\"discount\"))\n",
    ")\n",
    "\n",
    "# Show results\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "# Explain the execution plan\n",
    "result_df.explain(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1f5342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "def generate_test_data(spark, names,num_groups=3, overlap=True):\n",
    "    \"\"\"\n",
    "    Generate a list of DataFrames with potential overlapping 'UCIDs' in 'col0'.\n",
    "    \n",
    "    Parameters:\n",
    "        spark (SparkSession): The Spark session object.\n",
    "        num_cohorts (int): Number of cohorts (DataFrames) to generate.\n",
    "        overlap (bool): If True, allow overlap of UCIDs between cohorts.\n",
    "        \n",
    "    Returns:\n",
    "        List[DataFrame]: A list of PySpark DataFrames with 'UCID' overlap.\n",
    "    \"\"\"\n",
    "    df_list = []\n",
    "    \n",
    "    base_names = names\n",
    "    \n",
    "    for i in range(num_groups):\n",
    "        # Generate a subset of names with potential overlap\n",
    "        if overlap:\n",
    "            names = base_names[:i+2]  # Increasing overlap with each cohort\n",
    "        else:\n",
    "            ucids = [f'Person_{i}_{j}' for j in range(num_groups)]\n",
    "        \n",
    "        # Create a DataFrame for the group\n",
    "        group_df = spark.createDataFrame([(name, f'Group_{i}') for name in names], [\"names\", \"group_id\"])\n",
    "        df_list.append(group_df)\n",
    "    \n",
    "    return df_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10628db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = person_list = [\"Person_\"+str(i) for i in range(100)]\n",
    "test_data = generate_test_data(spark, names)\n",
    "df = reduce(DataFrame.unionByName, test_data).cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b0acde0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:=======================================>                (17 + 7) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|   names|group_id|\n",
      "+--------+--------+\n",
      "|Person_0| Group_0|\n",
      "|Person_1| Group_0|\n",
      "|Person_0| Group_1|\n",
      "|Person_1| Group_1|\n",
      "|Person_2| Group_1|\n",
      "|Person_0| Group_2|\n",
      "|Person_1| Group_2|\n",
      "|Person_2| Group_2|\n",
      "|Person_3| Group_2|\n",
      "+--------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "37baa1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- InMemoryTableScan [names#186, group_id#187]\n",
      "      +- InMemoryRelation [names#186, group_id#187], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "            +- Union\n",
      "               :- *(1) Scan ExistingRDD[names#186,group_id#187]\n",
      "               :- *(2) Scan ExistingRDD[names#190,group_id#191]\n",
      "               +- *(3) Scan ExistingRDD[names#194,group_id#195]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b5c736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy(\"group_id\").agg(sf.count(\"*\").alias(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab35af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[group_id#141], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(group_id#141, 200), ENSURE_REQUIREMENTS, [plan_id=397]\n",
      "      +- HashAggregate(keys=[group_id#141], functions=[partial_count(1)])\n",
      "         +- Union\n",
      "            :- Project [group_id#141]\n",
      "            :  +- Scan ExistingRDD[names#140,group_id#141]\n",
      "            :- Project [group_id#145]\n",
      "            :  +- Scan ExistingRDD[names#144,group_id#145]\n",
      "            +- Project [group_id#149]\n",
      "               +- Scan ExistingRDD[names#148,group_id#149]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "22b40117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:============================================>           (19 + 5) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|group_id|count|\n",
      "+--------+-----+\n",
      "| Group_1|    3|\n",
      "| Group_2|    4|\n",
      "| Group_0|    2|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbce55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
